{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Second Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "didWANszUlFe",
        "kwiwE9-tWI7f",
        "BjZp3M1OWqFL",
        "YhFnWlzgXLv6",
        "oGR8-DeXaXSu",
        "z1hRzacVajm9",
        "X196WbBtazil",
        "ieD9PZ_Gbba2",
        "jE3cGtqtb0X1",
        "B-0IK5TFcBqL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "693eab76-5740-4e25-d386-8a812f359c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "234cd74c-476a-40cb-dbfd-64b3e16ab6d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "4ffc9b2a-559c-4e59-89e8-ae637af2a782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[12])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe196156898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ERfiAowg8E9X+RAYL329OsCgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "a9f0a2f7-d6cd-4702-c85e-c6c69a614412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "79f44f8d-7cfe-403f-ca76-be8da1c4160a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz16GRyU_isG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a9ccbe50-f9ee-4b60-97eb-d6220912413b"
      },
      "source": [
        "from keras.layers import Activation, BatchNormalization, Dropout\n",
        "model = Sequential()\n",
        "\n",
        "# After 1x1 BN added + mp rm1 \n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(8, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(8, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # mp-rm1\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(8, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(8, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2))) # mp-rm2\n",
        "\n",
        "model.add(Convolution2D(20, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1))\n",
        "model.add(BatchNormalization()) # **\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 6))\n",
        "model.add(BatchNormalization()) # *1\n",
        "# model.add(Dropout(0.1)) # exp dropout\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "48ef3330-70eb-4de4-a07e-4ea2e14c1c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1385
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_78 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_79 (Conv2D)           (None, 26, 26, 8)         136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_80 (Conv2D)           (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_80 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_81 (Conv2D)           (None, 24, 24, 8)         136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_81 (Batc (None, 24, 24, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 24, 24, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_82 (Conv2D)           (None, 10, 10, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_82 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_83 (Conv2D)           (None, 10, 10, 8)         136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 10, 10, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 10, 10, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_84 (Conv2D)           (None, 8, 8, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_85 (Conv2D)           (None, 8, 8, 8)           136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 8, 8, 8)           32        \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_86 (Conv2D)           (None, 6, 6, 20)          1460      \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 6, 6, 20)          80        \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 6, 6, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_87 (Conv2D)           (None, 6, 6, 10)          210       \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 6, 6, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_88 (Conv2D)           (None, 1, 1, 10)          3610      \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 10,032\n",
            "Trainable params: 9,760\n",
            "Non-trainable params: 272\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq_wvwLxcrYS",
        "colab_type": "text"
      },
      "source": [
        "## All the run's below are to see the affect of BN, DP after certain layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "didWANszUlFe",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of second max pool in model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0QLlIbK0qAD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1227
        },
        "outputId": "fca4e60d-dde9-469e-9c0a-122a90b4189c"
      },
      "source": [
        "# commenting second max pool | mp-rm2\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=30, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 32s 529us/step - loss: 0.4395 - acc: 0.9146 - val_loss: 0.1335 - val_acc: 0.9814\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 29s 476us/step - loss: 0.1539 - acc: 0.9693 - val_loss: 0.0581 - val_acc: 0.9884\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 28s 471us/step - loss: 0.1027 - acc: 0.9776 - val_loss: 0.0700 - val_acc: 0.9862\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 27s 455us/step - loss: 0.0803 - acc: 0.9820 - val_loss: 0.0389 - val_acc: 0.9892\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 27s 453us/step - loss: 0.0656 - acc: 0.9840 - val_loss: 0.0376 - val_acc: 0.9904\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 28s 470us/step - loss: 0.0563 - acc: 0.9862 - val_loss: 0.0307 - val_acc: 0.9914\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0477 - acc: 0.9878 - val_loss: 0.0321 - val_acc: 0.9923\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 27s 454us/step - loss: 0.0464 - acc: 0.9884 - val_loss: 0.0296 - val_acc: 0.9915\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 28s 472us/step - loss: 0.0397 - acc: 0.9897 - val_loss: 0.0318 - val_acc: 0.9895\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 27s 453us/step - loss: 0.0361 - acc: 0.9907 - val_loss: 0.0279 - val_acc: 0.9917\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0340 - acc: 0.9908 - val_loss: 0.0256 - val_acc: 0.9926\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0313 - acc: 0.9914 - val_loss: 0.0275 - val_acc: 0.9912\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 28s 475us/step - loss: 0.0293 - acc: 0.9920 - val_loss: 0.0360 - val_acc: 0.9892\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 27s 448us/step - loss: 0.0278 - acc: 0.9922 - val_loss: 0.0251 - val_acc: 0.9938\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 28s 467us/step - loss: 0.0271 - acc: 0.9926 - val_loss: 0.0237 - val_acc: 0.9930\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 27s 446us/step - loss: 0.0250 - acc: 0.9929 - val_loss: 0.0246 - val_acc: 0.9920\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 27s 452us/step - loss: 0.0242 - acc: 0.9935 - val_loss: 0.0254 - val_acc: 0.9917\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 27s 458us/step - loss: 0.0217 - acc: 0.9936 - val_loss: 0.0300 - val_acc: 0.9900\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 27s 445us/step - loss: 0.0225 - acc: 0.9936 - val_loss: 0.0226 - val_acc: 0.9932\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 27s 452us/step - loss: 0.0207 - acc: 0.9941 - val_loss: 0.0263 - val_acc: 0.9914\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 28s 459us/step - loss: 0.0205 - acc: 0.9942 - val_loss: 0.0238 - val_acc: 0.9926\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 27s 445us/step - loss: 0.0206 - acc: 0.9940 - val_loss: 0.0235 - val_acc: 0.9922\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0184 - acc: 0.9948 - val_loss: 0.0242 - val_acc: 0.9923\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 29s 475us/step - loss: 0.0183 - acc: 0.9948 - val_loss: 0.0229 - val_acc: 0.9924\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 27s 454us/step - loss: 0.0176 - acc: 0.9950 - val_loss: 0.0237 - val_acc: 0.9923\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 27s 458us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0273 - val_acc: 0.9918\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0169 - acc: 0.9950 - val_loss: 0.0240 - val_acc: 0.9925\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 27s 446us/step - loss: 0.0146 - acc: 0.9955 - val_loss: 0.0259 - val_acc: 0.9913\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 28s 459us/step - loss: 0.0157 - acc: 0.9954 - val_loss: 0.0231 - val_acc: 0.9930\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 27s 449us/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0241 - val_acc: 0.9923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe196145e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwiwE9-tWI7f",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of adding Drop out after every BN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iPO6NsDA8he",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1154
        },
        "outputId": "e42af817-82c0-4086-9c34-f39fcaf2e1ad"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=30, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 35s 582us/step - loss: 0.5910 - acc: 0.8625 - val_loss: 0.1370 - val_acc: 0.9749\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.2199 - acc: 0.9491 - val_loss: 0.0619 - val_acc: 0.9857\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.1550 - acc: 0.9616 - val_loss: 0.0542 - val_acc: 0.9846\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 30s 508us/step - loss: 0.1252 - acc: 0.9680 - val_loss: 0.0541 - val_acc: 0.9854\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 32s 537us/step - loss: 0.1032 - acc: 0.9731 - val_loss: 0.0425 - val_acc: 0.9881\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 30s 505us/step - loss: 0.0945 - acc: 0.9753 - val_loss: 0.0326 - val_acc: 0.9911\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 31s 523us/step - loss: 0.0859 - acc: 0.9771 - val_loss: 0.0344 - val_acc: 0.9893\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.0815 - acc: 0.9780 - val_loss: 0.0323 - val_acc: 0.9899\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0738 - acc: 0.9798 - val_loss: 0.0269 - val_acc: 0.9914\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 32s 525us/step - loss: 0.0705 - acc: 0.9807 - val_loss: 0.0328 - val_acc: 0.9910\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.0662 - acc: 0.9818 - val_loss: 0.0299 - val_acc: 0.9911\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0636 - acc: 0.9824 - val_loss: 0.0266 - val_acc: 0.9917\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 0.0612 - acc: 0.9828 - val_loss: 0.0295 - val_acc: 0.9913\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 30s 507us/step - loss: 0.0596 - acc: 0.9830 - val_loss: 0.0286 - val_acc: 0.9913\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 33s 543us/step - loss: 0.0586 - acc: 0.9836 - val_loss: 0.0246 - val_acc: 0.9929\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 30s 506us/step - loss: 0.0562 - acc: 0.9839 - val_loss: 0.0264 - val_acc: 0.9923\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0557 - acc: 0.9838 - val_loss: 0.0236 - val_acc: 0.9922\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 32s 528us/step - loss: 0.0537 - acc: 0.9845 - val_loss: 0.0246 - val_acc: 0.9928\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0509 - acc: 0.9851 - val_loss: 0.0238 - val_acc: 0.9927\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 31s 525us/step - loss: 0.0534 - acc: 0.9848 - val_loss: 0.0239 - val_acc: 0.9927\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 30s 499us/step - loss: 0.0501 - acc: 0.9856 - val_loss: 0.0244 - val_acc: 0.9923\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 0.0505 - acc: 0.9854 - val_loss: 0.0234 - val_acc: 0.9932\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.0496 - acc: 0.9858 - val_loss: 0.0212 - val_acc: 0.9936\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.0494 - acc: 0.9852 - val_loss: 0.0224 - val_acc: 0.9932\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 32s 538us/step - loss: 0.0483 - acc: 0.9861 - val_loss: 0.0241 - val_acc: 0.9926\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 31s 509us/step - loss: 0.0468 - acc: 0.9864 - val_loss: 0.0232 - val_acc: 0.9938\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.0470 - acc: 0.9863 - val_loss: 0.0201 - val_acc: 0.9940\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 31s 523us/step - loss: 0.0443 - acc: 0.9871 - val_loss: 0.0201 - val_acc: 0.9939\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 30s 505us/step - loss: 0.0437 - acc: 0.9875 - val_loss: 0.0223 - val_acc: 0.9924\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0447 - acc: 0.9870 - val_loss: 0.0210 - val_acc: 0.9932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe14e70ec88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjZp3M1OWqFL",
        "colab_type": "text"
      },
      "source": [
        "### Previous fit was DP after every BN but not before flatten, so this run is to check what's the effect of adding Dropout before flatten layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qFVGJy4ODN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1154
        },
        "outputId": "e49b4500-2c39-43c3-e6cf-27576a273b9a"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=30, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 36s 602us/step - loss: 0.7147 - acc: 0.7904 - val_loss: 0.1467 - val_acc: 0.9730\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.3994 - acc: 0.8694 - val_loss: 0.1052 - val_acc: 0.9772\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 31s 522us/step - loss: 0.3231 - acc: 0.8906 - val_loss: 0.0684 - val_acc: 0.9835\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 33s 542us/step - loss: 0.2816 - acc: 0.9043 - val_loss: 0.0600 - val_acc: 0.9842\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 33s 548us/step - loss: 0.2549 - acc: 0.9127 - val_loss: 0.0459 - val_acc: 0.9872\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.2330 - acc: 0.9197 - val_loss: 0.0433 - val_acc: 0.9885\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 32s 529us/step - loss: 0.2216 - acc: 0.9244 - val_loss: 0.0355 - val_acc: 0.9895\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.2061 - acc: 0.9286 - val_loss: 0.0361 - val_acc: 0.9894\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 32s 542us/step - loss: 0.2001 - acc: 0.9289 - val_loss: 0.0322 - val_acc: 0.9907\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.1948 - acc: 0.9300 - val_loss: 0.0303 - val_acc: 0.9911\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 32s 540us/step - loss: 0.1918 - acc: 0.9309 - val_loss: 0.0308 - val_acc: 0.9910\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.1854 - acc: 0.9331 - val_loss: 0.0316 - val_acc: 0.9901\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.1822 - acc: 0.9338 - val_loss: 0.0288 - val_acc: 0.9916\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 32s 539us/step - loss: 0.1814 - acc: 0.9330 - val_loss: 0.0320 - val_acc: 0.9904\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 33s 546us/step - loss: 0.1751 - acc: 0.9350 - val_loss: 0.0296 - val_acc: 0.9904\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.1764 - acc: 0.9351 - val_loss: 0.0250 - val_acc: 0.9919\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 32s 535us/step - loss: 0.1705 - acc: 0.9379 - val_loss: 0.0242 - val_acc: 0.9928\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.1679 - acc: 0.9373 - val_loss: 0.0259 - val_acc: 0.9919\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 32s 542us/step - loss: 0.1661 - acc: 0.9370 - val_loss: 0.0289 - val_acc: 0.9911\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.1604 - acc: 0.9378 - val_loss: 0.0276 - val_acc: 0.9919\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.1613 - acc: 0.9389 - val_loss: 0.0297 - val_acc: 0.9899\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 33s 543us/step - loss: 0.1618 - acc: 0.9388 - val_loss: 0.0284 - val_acc: 0.9903\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 31s 522us/step - loss: 0.1593 - acc: 0.9399 - val_loss: 0.0260 - val_acc: 0.9919\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 33s 546us/step - loss: 0.1573 - acc: 0.9396 - val_loss: 0.0232 - val_acc: 0.9927\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 33s 549us/step - loss: 0.1544 - acc: 0.9400 - val_loss: 0.0257 - val_acc: 0.9921\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 31s 522us/step - loss: 0.1538 - acc: 0.9412 - val_loss: 0.0246 - val_acc: 0.9921\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 34s 560us/step - loss: 0.1507 - acc: 0.9418 - val_loss: 0.0248 - val_acc: 0.9922\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.1478 - acc: 0.9422 - val_loss: 0.0253 - val_acc: 0.9924\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 33s 553us/step - loss: 0.1493 - acc: 0.9423 - val_loss: 0.0244 - val_acc: 0.9925\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 33s 546us/step - loss: 0.1521 - acc: 0.9410 - val_loss: 0.0225 - val_acc: 0.9921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe148dbfb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhFnWlzgXLv6",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of first max pool layer in model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51jjs3sr7gZt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1865
        },
        "outputId": "f3288a0c-cd90-4ca5-fd76-4a6fd12ef9a1"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 63s 1ms/step - loss: 0.3982 - acc: 0.9305 - val_loss: 0.1351 - val_acc: 0.9821\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 56s 940us/step - loss: 0.1627 - acc: 0.9677 - val_loss: 0.0730 - val_acc: 0.9851\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 0.1128 - acc: 0.9746 - val_loss: 0.0563 - val_acc: 0.9861\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 56s 937us/step - loss: 0.0853 - acc: 0.9794 - val_loss: 0.0424 - val_acc: 0.9903\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 56s 931us/step - loss: 0.0702 - acc: 0.9830 - val_loss: 0.0384 - val_acc: 0.9903\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 57s 944us/step - loss: 0.0613 - acc: 0.9849 - val_loss: 0.0332 - val_acc: 0.9909\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 0.0532 - acc: 0.9863 - val_loss: 0.0335 - val_acc: 0.9904\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 56s 942us/step - loss: 0.0486 - acc: 0.9872 - val_loss: 0.0347 - val_acc: 0.9905\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 0.0431 - acc: 0.9890 - val_loss: 0.0410 - val_acc: 0.9890\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 56s 934us/step - loss: 0.0404 - acc: 0.9894 - val_loss: 0.0318 - val_acc: 0.9908\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 56s 941us/step - loss: 0.0363 - acc: 0.9899 - val_loss: 0.0340 - val_acc: 0.9903\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 57s 944us/step - loss: 0.0346 - acc: 0.9906 - val_loss: 0.0304 - val_acc: 0.9912\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 0.0326 - acc: 0.9913 - val_loss: 0.0348 - val_acc: 0.9904\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 0.0302 - acc: 0.9919 - val_loss: 0.0328 - val_acc: 0.9899\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 56s 937us/step - loss: 0.0298 - acc: 0.9922 - val_loss: 0.0411 - val_acc: 0.9881\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 56s 936us/step - loss: 0.0274 - acc: 0.9925 - val_loss: 0.0375 - val_acc: 0.9881\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 57s 946us/step - loss: 0.0261 - acc: 0.9929 - val_loss: 0.0315 - val_acc: 0.9905\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 57s 944us/step - loss: 0.0238 - acc: 0.9934 - val_loss: 0.0328 - val_acc: 0.9895\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 57s 942us/step - loss: 0.0238 - acc: 0.9934 - val_loss: 0.0292 - val_acc: 0.9911\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 56s 941us/step - loss: 0.0230 - acc: 0.9933 - val_loss: 0.0298 - val_acc: 0.9916\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 56s 937us/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.0338 - val_acc: 0.9895\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 57s 947us/step - loss: 0.0214 - acc: 0.9938 - val_loss: 0.0313 - val_acc: 0.9899\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 57s 946us/step - loss: 0.0200 - acc: 0.9945 - val_loss: 0.0319 - val_acc: 0.9905\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 0.0189 - acc: 0.9944 - val_loss: 0.0319 - val_acc: 0.9899\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 57s 946us/step - loss: 0.0174 - acc: 0.9949 - val_loss: 0.0331 - val_acc: 0.9904\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 56s 933us/step - loss: 0.0180 - acc: 0.9950 - val_loss: 0.0314 - val_acc: 0.9909\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 56s 937us/step - loss: 0.0163 - acc: 0.9955 - val_loss: 0.0291 - val_acc: 0.9916\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 57s 946us/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.0374 - val_acc: 0.9884\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 57s 944us/step - loss: 0.0170 - acc: 0.9950 - val_loss: 0.0331 - val_acc: 0.9901\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 56s 942us/step - loss: 0.0160 - acc: 0.9957 - val_loss: 0.0295 - val_acc: 0.9915\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 56s 934us/step - loss: 0.0147 - acc: 0.9959 - val_loss: 0.0302 - val_acc: 0.9914\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 57s 947us/step - loss: 0.0151 - acc: 0.9958 - val_loss: 0.0371 - val_acc: 0.9888\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 57s 949us/step - loss: 0.0137 - acc: 0.9960 - val_loss: 0.0305 - val_acc: 0.9912\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 57s 944us/step - loss: 0.0141 - acc: 0.9960 - val_loss: 0.0352 - val_acc: 0.9902\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 57s 947us/step - loss: 0.0129 - acc: 0.9963 - val_loss: 0.0309 - val_acc: 0.9917\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 57s 942us/step - loss: 0.0138 - acc: 0.9959 - val_loss: 0.0341 - val_acc: 0.9895\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 56s 940us/step - loss: 0.0136 - acc: 0.9957 - val_loss: 0.0312 - val_acc: 0.9913\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 56s 940us/step - loss: 0.0119 - acc: 0.9967 - val_loss: 0.0348 - val_acc: 0.9912\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 57s 942us/step - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0335 - val_acc: 0.9914\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 57s 945us/step - loss: 0.0125 - acc: 0.9966 - val_loss: 0.0396 - val_acc: 0.9887\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 0.0126 - acc: 0.9963 - val_loss: 0.0325 - val_acc: 0.9905\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 56s 938us/step - loss: 0.0112 - acc: 0.9968 - val_loss: 0.0360 - val_acc: 0.9901\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 56s 940us/step - loss: 0.0106 - acc: 0.9971 - val_loss: 0.0282 - val_acc: 0.9912\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 57s 948us/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.0313 - val_acc: 0.9905\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 57s 948us/step - loss: 0.0097 - acc: 0.9974 - val_loss: 0.0290 - val_acc: 0.9917\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 57s 946us/step - loss: 0.0109 - acc: 0.9969 - val_loss: 0.0341 - val_acc: 0.9895\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 57s 943us/step - loss: 0.0100 - acc: 0.9973 - val_loss: 0.0335 - val_acc: 0.9911\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 56s 938us/step - loss: 0.0100 - acc: 0.9972 - val_loss: 0.0358 - val_acc: 0.9898\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 57s 946us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.0327 - val_acc: 0.9914\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 57s 947us/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.0363 - val_acc: 0.9900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f907d230630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGR8-DeXaXSu",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of second max pool in model ran for 50 ep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOaV2a2Q0yg5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1865
        },
        "outputId": "383b84ba-f92d-4ad1-a38c-965ecd2eab1f"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.4477 - acc: 0.9105 - val_loss: 0.1376 - val_acc: 0.9784\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 27s 449us/step - loss: 0.1531 - acc: 0.9694 - val_loss: 0.0686 - val_acc: 0.9863\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 28s 465us/step - loss: 0.1044 - acc: 0.9771 - val_loss: 0.0490 - val_acc: 0.9890\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 29s 483us/step - loss: 0.0782 - acc: 0.9823 - val_loss: 0.0419 - val_acc: 0.9896\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0647 - acc: 0.9839 - val_loss: 0.0433 - val_acc: 0.9874\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0555 - acc: 0.9861 - val_loss: 0.0354 - val_acc: 0.9890\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 28s 471us/step - loss: 0.0479 - acc: 0.9881 - val_loss: 0.0339 - val_acc: 0.9897\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0434 - acc: 0.9885 - val_loss: 0.0341 - val_acc: 0.9907\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0397 - acc: 0.9896 - val_loss: 0.0325 - val_acc: 0.9909\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 28s 468us/step - loss: 0.0373 - acc: 0.9898 - val_loss: 0.0283 - val_acc: 0.9908\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 27s 449us/step - loss: 0.0366 - acc: 0.9898 - val_loss: 0.0293 - val_acc: 0.9915\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 27s 448us/step - loss: 0.0318 - acc: 0.9917 - val_loss: 0.0344 - val_acc: 0.9903\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 28s 468us/step - loss: 0.0295 - acc: 0.9921 - val_loss: 0.0280 - val_acc: 0.9923\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 28s 459us/step - loss: 0.0295 - acc: 0.9918 - val_loss: 0.0292 - val_acc: 0.9908\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 29s 476us/step - loss: 0.0266 - acc: 0.9929 - val_loss: 0.0332 - val_acc: 0.9907\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 28s 466us/step - loss: 0.0246 - acc: 0.9933 - val_loss: 0.0311 - val_acc: 0.9897\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 27s 454us/step - loss: 0.0248 - acc: 0.9932 - val_loss: 0.0295 - val_acc: 0.9915\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 28s 459us/step - loss: 0.0219 - acc: 0.9940 - val_loss: 0.0277 - val_acc: 0.9906\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0219 - acc: 0.9941 - val_loss: 0.0396 - val_acc: 0.9880\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0204 - acc: 0.9942 - val_loss: 0.0302 - val_acc: 0.9901\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 28s 464us/step - loss: 0.0200 - acc: 0.9944 - val_loss: 0.0285 - val_acc: 0.9916\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0192 - acc: 0.9944 - val_loss: 0.0313 - val_acc: 0.9911\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0197 - acc: 0.9946 - val_loss: 0.0302 - val_acc: 0.9907\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 28s 470us/step - loss: 0.0180 - acc: 0.9950 - val_loss: 0.0255 - val_acc: 0.9913\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 28s 467us/step - loss: 0.0175 - acc: 0.9949 - val_loss: 0.0283 - val_acc: 0.9912\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 27s 458us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0276 - val_acc: 0.9915\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 29s 486us/step - loss: 0.0164 - acc: 0.9952 - val_loss: 0.0317 - val_acc: 0.9905\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 27s 447us/step - loss: 0.0166 - acc: 0.9956 - val_loss: 0.0276 - val_acc: 0.9920\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 27s 448us/step - loss: 0.0145 - acc: 0.9959 - val_loss: 0.0327 - val_acc: 0.9900\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 28s 468us/step - loss: 0.0150 - acc: 0.9957 - val_loss: 0.0299 - val_acc: 0.9907\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0143 - acc: 0.9957 - val_loss: 0.0317 - val_acc: 0.9908\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0130 - acc: 0.9962 - val_loss: 0.0297 - val_acc: 0.9914\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 28s 470us/step - loss: 0.0129 - acc: 0.9962 - val_loss: 0.0384 - val_acc: 0.9895\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.0276 - val_acc: 0.9917\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0281 - val_acc: 0.9916\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 28s 471us/step - loss: 0.0122 - acc: 0.9964 - val_loss: 0.0314 - val_acc: 0.9908\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 27s 448us/step - loss: 0.0117 - acc: 0.9967 - val_loss: 0.0314 - val_acc: 0.9913\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 29s 476us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.0304 - val_acc: 0.9913\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 28s 470us/step - loss: 0.0124 - acc: 0.9965 - val_loss: 0.0321 - val_acc: 0.9900\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0309 - val_acc: 0.9908\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0110 - acc: 0.9971 - val_loss: 0.0347 - val_acc: 0.9897\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 28s 469us/step - loss: 0.0109 - acc: 0.9967 - val_loss: 0.0306 - val_acc: 0.9914\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 27s 452us/step - loss: 0.0111 - acc: 0.9964 - val_loss: 0.0329 - val_acc: 0.9906\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 27s 452us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0313 - val_acc: 0.9917\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 28s 473us/step - loss: 0.0093 - acc: 0.9975 - val_loss: 0.0304 - val_acc: 0.9912\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 27s 454us/step - loss: 0.0103 - acc: 0.9971 - val_loss: 0.0327 - val_acc: 0.9917\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 28s 466us/step - loss: 0.0097 - acc: 0.9972 - val_loss: 0.0306 - val_acc: 0.9919\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 28s 473us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0335 - val_acc: 0.9909\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0098 - acc: 0.9973 - val_loss: 0.0320 - val_acc: 0.9909\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 28s 467us/step - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0285 - val_acc: 0.9918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe164642940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hRzacVajm9",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of adding Drop out after every BN for 50 ep -- 25 th ep 0.9946 val acc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2VvP33LBaxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1865
        },
        "outputId": "7f496c50-d965-45f0-8b58-5acd4afc8f4e"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 35s 590us/step - loss: 0.5683 - acc: 0.8706 - val_loss: 0.1472 - val_acc: 0.9710\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.2134 - acc: 0.9509 - val_loss: 0.0665 - val_acc: 0.9850\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 32s 540us/step - loss: 0.1529 - acc: 0.9621 - val_loss: 0.0601 - val_acc: 0.9855\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.1265 - acc: 0.9680 - val_loss: 0.0431 - val_acc: 0.9884\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.1091 - acc: 0.9712 - val_loss: 0.0404 - val_acc: 0.9895\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 32s 540us/step - loss: 0.0946 - acc: 0.9759 - val_loss: 0.0343 - val_acc: 0.9902\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.0870 - acc: 0.9766 - val_loss: 0.0505 - val_acc: 0.9844\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 32s 536us/step - loss: 0.0802 - acc: 0.9776 - val_loss: 0.0314 - val_acc: 0.9903\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 32s 540us/step - loss: 0.0750 - acc: 0.9797 - val_loss: 0.0269 - val_acc: 0.9917\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0705 - acc: 0.9803 - val_loss: 0.0292 - val_acc: 0.9913\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 32s 536us/step - loss: 0.0669 - acc: 0.9820 - val_loss: 0.0294 - val_acc: 0.9918\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.0638 - acc: 0.9826 - val_loss: 0.0256 - val_acc: 0.9918\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.0621 - acc: 0.9830 - val_loss: 0.0302 - val_acc: 0.9905\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.0614 - acc: 0.9831 - val_loss: 0.0247 - val_acc: 0.9929\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 32s 528us/step - loss: 0.0592 - acc: 0.9829 - val_loss: 0.0245 - val_acc: 0.9935\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.0580 - acc: 0.9831 - val_loss: 0.0241 - val_acc: 0.9933\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 0.0569 - acc: 0.9842 - val_loss: 0.0239 - val_acc: 0.9924\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.0530 - acc: 0.9844 - val_loss: 0.0215 - val_acc: 0.9933\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.0542 - acc: 0.9841 - val_loss: 0.0237 - val_acc: 0.9927\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 0.0511 - acc: 0.9853 - val_loss: 0.0231 - val_acc: 0.9925\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 32s 537us/step - loss: 0.0507 - acc: 0.9853 - val_loss: 0.0228 - val_acc: 0.9931\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 0.0503 - acc: 0.9857 - val_loss: 0.0230 - val_acc: 0.9931\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.0480 - acc: 0.9859 - val_loss: 0.0195 - val_acc: 0.9938\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 32s 526us/step - loss: 0.0490 - acc: 0.9857 - val_loss: 0.0213 - val_acc: 0.9934\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 32s 532us/step - loss: 0.0482 - acc: 0.9855 - val_loss: 0.0191 - val_acc: 0.9946\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 32s 535us/step - loss: 0.0448 - acc: 0.9868 - val_loss: 0.0194 - val_acc: 0.9938\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0474 - acc: 0.9863 - val_loss: 0.0215 - val_acc: 0.9926\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 32s 537us/step - loss: 0.0468 - acc: 0.9868 - val_loss: 0.0254 - val_acc: 0.9926\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.0453 - acc: 0.9868 - val_loss: 0.0204 - val_acc: 0.9946\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.0436 - acc: 0.9875 - val_loss: 0.0206 - val_acc: 0.9936\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.0442 - acc: 0.9868 - val_loss: 0.0226 - val_acc: 0.9933\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.0438 - acc: 0.9869 - val_loss: 0.0213 - val_acc: 0.9937\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.0434 - acc: 0.9878 - val_loss: 0.0218 - val_acc: 0.9930\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 32s 537us/step - loss: 0.0432 - acc: 0.9873 - val_loss: 0.0197 - val_acc: 0.9936\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 31s 520us/step - loss: 0.0447 - acc: 0.9863 - val_loss: 0.0209 - val_acc: 0.9939\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 32s 537us/step - loss: 0.0424 - acc: 0.9877 - val_loss: 0.0219 - val_acc: 0.9936\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.0411 - acc: 0.9877 - val_loss: 0.0237 - val_acc: 0.9930\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 32s 539us/step - loss: 0.0413 - acc: 0.9879 - val_loss: 0.0201 - val_acc: 0.9939\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.0401 - acc: 0.9884 - val_loss: 0.0199 - val_acc: 0.9935\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.0402 - acc: 0.9878 - val_loss: 0.0208 - val_acc: 0.9932\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.0391 - acc: 0.9880 - val_loss: 0.0248 - val_acc: 0.9919\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 31s 509us/step - loss: 0.0410 - acc: 0.9878 - val_loss: 0.0207 - val_acc: 0.9935\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.0406 - acc: 0.9881 - val_loss: 0.0211 - val_acc: 0.9936\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 33s 544us/step - loss: 0.0389 - acc: 0.9889 - val_loss: 0.0211 - val_acc: 0.9934\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 0.0392 - acc: 0.9887 - val_loss: 0.0188 - val_acc: 0.9941\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 31s 524us/step - loss: 0.0377 - acc: 0.9885 - val_loss: 0.0189 - val_acc: 0.9939\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.0367 - acc: 0.9882 - val_loss: 0.0200 - val_acc: 0.9937\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 32s 536us/step - loss: 0.0387 - acc: 0.9879 - val_loss: 0.0179 - val_acc: 0.9943\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.0379 - acc: 0.9884 - val_loss: 0.0185 - val_acc: 0.9930\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.0366 - acc: 0.9889 - val_loss: 0.0197 - val_acc: 0.9935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe14c2c7d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X196WbBtazil",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of adding Drop out after every BN for BS 64 + 50 ep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npc_fps0yklP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1865
        },
        "outputId": "4af5b6e6-1cdb-46a3-e176-7b1f6e26c0eb"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.7009 - acc: 0.8417 - val_loss: 0.2148 - val_acc: 0.9642\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.2441 - acc: 0.9544 - val_loss: 0.1143 - val_acc: 0.9763\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.1605 - acc: 0.9662 - val_loss: 0.0806 - val_acc: 0.9796\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.1224 - acc: 0.9714 - val_loss: 0.0673 - val_acc: 0.9831\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.1025 - acc: 0.9743 - val_loss: 0.0461 - val_acc: 0.9882\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0896 - acc: 0.9772 - val_loss: 0.0400 - val_acc: 0.9880\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0820 - acc: 0.9787 - val_loss: 0.0378 - val_acc: 0.9900\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0736 - acc: 0.9798 - val_loss: 0.0344 - val_acc: 0.9900\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0693 - acc: 0.9805 - val_loss: 0.0287 - val_acc: 0.9912\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0626 - acc: 0.9827 - val_loss: 0.0302 - val_acc: 0.9903\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0620 - acc: 0.9823 - val_loss: 0.0291 - val_acc: 0.9908\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0586 - acc: 0.9834 - val_loss: 0.0354 - val_acc: 0.9891\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0555 - acc: 0.9844 - val_loss: 0.0310 - val_acc: 0.9903\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 17s 287us/step - loss: 0.0554 - acc: 0.9839 - val_loss: 0.0267 - val_acc: 0.9918\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0516 - acc: 0.9849 - val_loss: 0.0281 - val_acc: 0.9910\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 17s 283us/step - loss: 0.0504 - acc: 0.9852 - val_loss: 0.0276 - val_acc: 0.9924\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.0492 - acc: 0.9856 - val_loss: 0.0268 - val_acc: 0.9917\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0494 - acc: 0.9857 - val_loss: 0.0248 - val_acc: 0.9927\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0478 - acc: 0.9858 - val_loss: 0.0234 - val_acc: 0.9933\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0453 - acc: 0.9867 - val_loss: 0.0236 - val_acc: 0.9935\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0462 - acc: 0.9861 - val_loss: 0.0254 - val_acc: 0.9931\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0435 - acc: 0.9873 - val_loss: 0.0243 - val_acc: 0.9929\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0445 - acc: 0.9868 - val_loss: 0.0211 - val_acc: 0.9945\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0423 - acc: 0.9871 - val_loss: 0.0251 - val_acc: 0.9932\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0417 - acc: 0.9876 - val_loss: 0.0256 - val_acc: 0.9926\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0407 - acc: 0.9881 - val_loss: 0.0220 - val_acc: 0.9933\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0408 - acc: 0.9881 - val_loss: 0.0229 - val_acc: 0.9924\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0396 - acc: 0.9881 - val_loss: 0.0218 - val_acc: 0.9934\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0380 - acc: 0.9889 - val_loss: 0.0226 - val_acc: 0.9927\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0391 - acc: 0.9884 - val_loss: 0.0207 - val_acc: 0.9936\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 18s 307us/step - loss: 0.0393 - acc: 0.9890 - val_loss: 0.0211 - val_acc: 0.9937\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 17s 285us/step - loss: 0.0369 - acc: 0.9887 - val_loss: 0.0214 - val_acc: 0.9934\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0391 - acc: 0.9885 - val_loss: 0.0209 - val_acc: 0.9938\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0377 - acc: 0.9886 - val_loss: 0.0223 - val_acc: 0.9940\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0369 - acc: 0.9888 - val_loss: 0.0211 - val_acc: 0.9933\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.0362 - acc: 0.9889 - val_loss: 0.0248 - val_acc: 0.9932\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0362 - acc: 0.9891 - val_loss: 0.0195 - val_acc: 0.9935\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0353 - acc: 0.9890 - val_loss: 0.0195 - val_acc: 0.9937\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0350 - acc: 0.9888 - val_loss: 0.0217 - val_acc: 0.9927\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.0339 - acc: 0.9897 - val_loss: 0.0207 - val_acc: 0.9938\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 17s 286us/step - loss: 0.0361 - acc: 0.9892 - val_loss: 0.0207 - val_acc: 0.9935\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0340 - acc: 0.9898 - val_loss: 0.0229 - val_acc: 0.9933\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0351 - acc: 0.9895 - val_loss: 0.0228 - val_acc: 0.9930\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0336 - acc: 0.9902 - val_loss: 0.0225 - val_acc: 0.9929\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.0330 - acc: 0.9898 - val_loss: 0.0212 - val_acc: 0.9934\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0330 - acc: 0.9897 - val_loss: 0.0219 - val_acc: 0.9936\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0338 - acc: 0.9901 - val_loss: 0.0245 - val_acc: 0.9936\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0329 - acc: 0.9899 - val_loss: 0.0227 - val_acc: 0.9930\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 17s 286us/step - loss: 0.0334 - acc: 0.9898 - val_loss: 0.0221 - val_acc: 0.9930\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 19s 309us/step - loss: 0.0315 - acc: 0.9907 - val_loss: 0.0208 - val_acc: 0.9946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1411e6978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieD9PZ_Gbba2",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of adding Drop out after every BN + BS 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYB7dZ0j2ndC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1865
        },
        "outputId": "8f210f15-b759-43d8-d7fa-f8ac13ca28ce"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=128, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.8918 - acc: 0.7896 - val_loss: 0.2606 - val_acc: 0.9600\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.3252 - acc: 0.9499 - val_loss: 0.1641 - val_acc: 0.9751\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.2119 - acc: 0.9642 - val_loss: 0.1057 - val_acc: 0.9805\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.1579 - acc: 0.9704 - val_loss: 0.0805 - val_acc: 0.9841\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 10s 161us/step - loss: 0.1271 - acc: 0.9750 - val_loss: 0.0638 - val_acc: 0.9865\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.1072 - acc: 0.9773 - val_loss: 0.0539 - val_acc: 0.9879\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0925 - acc: 0.9791 - val_loss: 0.0450 - val_acc: 0.9889\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 10s 164us/step - loss: 0.0832 - acc: 0.9806 - val_loss: 0.0394 - val_acc: 0.9904\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0748 - acc: 0.9819 - val_loss: 0.0357 - val_acc: 0.9909\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.0701 - acc: 0.9824 - val_loss: 0.0313 - val_acc: 0.9919\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.0640 - acc: 0.9828 - val_loss: 0.0330 - val_acc: 0.9909\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.0590 - acc: 0.9846 - val_loss: 0.0296 - val_acc: 0.9918\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.0581 - acc: 0.9849 - val_loss: 0.0312 - val_acc: 0.9913\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0547 - acc: 0.9847 - val_loss: 0.0272 - val_acc: 0.9922\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 10s 159us/step - loss: 0.0524 - acc: 0.9851 - val_loss: 0.0258 - val_acc: 0.9929\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0509 - acc: 0.9858 - val_loss: 0.0270 - val_acc: 0.9914\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0483 - acc: 0.9862 - val_loss: 0.0234 - val_acc: 0.9925\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 10s 163us/step - loss: 0.0457 - acc: 0.9872 - val_loss: 0.0295 - val_acc: 0.9907\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 10s 160us/step - loss: 0.0466 - acc: 0.9865 - val_loss: 0.0272 - val_acc: 0.9919\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0431 - acc: 0.9874 - val_loss: 0.0233 - val_acc: 0.9922\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0429 - acc: 0.9877 - val_loss: 0.0275 - val_acc: 0.9917\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0426 - acc: 0.9875 - val_loss: 0.0238 - val_acc: 0.9930\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0396 - acc: 0.9879 - val_loss: 0.0259 - val_acc: 0.9917\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0400 - acc: 0.9881 - val_loss: 0.0243 - val_acc: 0.9933\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0395 - acc: 0.9884 - val_loss: 0.0223 - val_acc: 0.9929\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0382 - acc: 0.9883 - val_loss: 0.0207 - val_acc: 0.9935\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 10s 163us/step - loss: 0.0374 - acc: 0.9890 - val_loss: 0.0204 - val_acc: 0.9933\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0373 - acc: 0.9889 - val_loss: 0.0228 - val_acc: 0.9930\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0378 - acc: 0.9887 - val_loss: 0.0215 - val_acc: 0.9931\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0380 - acc: 0.9886 - val_loss: 0.0225 - val_acc: 0.9925\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0365 - acc: 0.9892 - val_loss: 0.0214 - val_acc: 0.9925\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0358 - acc: 0.9896 - val_loss: 0.0196 - val_acc: 0.9940\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0354 - acc: 0.9895 - val_loss: 0.0220 - val_acc: 0.9933\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0338 - acc: 0.9901 - val_loss: 0.0221 - val_acc: 0.9932\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0345 - acc: 0.9893 - val_loss: 0.0181 - val_acc: 0.9938\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0338 - acc: 0.9895 - val_loss: 0.0193 - val_acc: 0.9933\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0345 - acc: 0.9894 - val_loss: 0.0208 - val_acc: 0.9928\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0338 - acc: 0.9897 - val_loss: 0.0193 - val_acc: 0.9940\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0339 - acc: 0.9896 - val_loss: 0.0214 - val_acc: 0.9939\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0320 - acc: 0.9904 - val_loss: 0.0176 - val_acc: 0.9936\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0326 - acc: 0.9903 - val_loss: 0.0214 - val_acc: 0.9932\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 10s 163us/step - loss: 0.0328 - acc: 0.9898 - val_loss: 0.0183 - val_acc: 0.9936\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 10s 160us/step - loss: 0.0312 - acc: 0.9906 - val_loss: 0.0212 - val_acc: 0.9927\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0329 - acc: 0.9895 - val_loss: 0.0255 - val_acc: 0.9920\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0334 - acc: 0.9898 - val_loss: 0.0194 - val_acc: 0.9937\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0314 - acc: 0.9905 - val_loss: 0.0233 - val_acc: 0.9924\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0320 - acc: 0.9899 - val_loss: 0.0200 - val_acc: 0.9935\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0317 - acc: 0.9903 - val_loss: 0.0205 - val_acc: 0.9932\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.0309 - acc: 0.9906 - val_loss: 0.0186 - val_acc: 0.9933\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0305 - acc: 0.9910 - val_loss: 0.0193 - val_acc: 0.9939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe13ee50748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE3cGtqtb0X1",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of adding Drop out after every BN + DP before flatten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmGLsKr0OXct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1865
        },
        "outputId": "ecf6de55-b3cc-4268-b446-a509d8a15298"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 37s 617us/step - loss: 0.7426 - acc: 0.7818 - val_loss: 0.1659 - val_acc: 0.9727\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 32s 539us/step - loss: 0.4092 - acc: 0.8668 - val_loss: 0.0979 - val_acc: 0.9813\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.3292 - acc: 0.8902 - val_loss: 0.0716 - val_acc: 0.9845\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 34s 565us/step - loss: 0.2925 - acc: 0.8999 - val_loss: 0.0614 - val_acc: 0.9857\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.2621 - acc: 0.9091 - val_loss: 0.0442 - val_acc: 0.9887\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 33s 548us/step - loss: 0.2388 - acc: 0.9167 - val_loss: 0.0580 - val_acc: 0.9839\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.2242 - acc: 0.9219 - val_loss: 0.0412 - val_acc: 0.9893\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.2174 - acc: 0.9237 - val_loss: 0.0335 - val_acc: 0.9910\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 34s 562us/step - loss: 0.2060 - acc: 0.9259 - val_loss: 0.0363 - val_acc: 0.9903\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 32s 533us/step - loss: 0.1985 - acc: 0.9276 - val_loss: 0.0351 - val_acc: 0.9897\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 33s 551us/step - loss: 0.1971 - acc: 0.9284 - val_loss: 0.0364 - val_acc: 0.9896\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 32s 532us/step - loss: 0.1922 - acc: 0.9310 - val_loss: 0.0385 - val_acc: 0.9883\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 32s 535us/step - loss: 0.1873 - acc: 0.9323 - val_loss: 0.0283 - val_acc: 0.9914\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 34s 562us/step - loss: 0.1794 - acc: 0.9332 - val_loss: 0.0287 - val_acc: 0.9907\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.1799 - acc: 0.9332 - val_loss: 0.0314 - val_acc: 0.9906\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 33s 550us/step - loss: 0.1761 - acc: 0.9347 - val_loss: 0.0325 - val_acc: 0.9901\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.1744 - acc: 0.9353 - val_loss: 0.0297 - val_acc: 0.9912\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 33s 552us/step - loss: 0.1730 - acc: 0.9350 - val_loss: 0.0294 - val_acc: 0.9908\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 33s 547us/step - loss: 0.1706 - acc: 0.9365 - val_loss: 0.0281 - val_acc: 0.9913\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.1672 - acc: 0.9369 - val_loss: 0.0305 - val_acc: 0.9912\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 33s 551us/step - loss: 0.1639 - acc: 0.9363 - val_loss: 0.0262 - val_acc: 0.9925\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 32s 532us/step - loss: 0.1622 - acc: 0.9395 - val_loss: 0.0264 - val_acc: 0.9916\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 33s 543us/step - loss: 0.1614 - acc: 0.9377 - val_loss: 0.0239 - val_acc: 0.9927\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 33s 552us/step - loss: 0.1595 - acc: 0.9377 - val_loss: 0.0272 - val_acc: 0.9917\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.1572 - acc: 0.9389 - val_loss: 0.0267 - val_acc: 0.9916\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 33s 553us/step - loss: 0.1592 - acc: 0.9373 - val_loss: 0.0261 - val_acc: 0.9925\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 32s 532us/step - loss: 0.1585 - acc: 0.9389 - val_loss: 0.0256 - val_acc: 0.9923\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 33s 555us/step - loss: 0.1552 - acc: 0.9399 - val_loss: 0.0254 - val_acc: 0.9917\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 33s 551us/step - loss: 0.1567 - acc: 0.9382 - val_loss: 0.0262 - val_acc: 0.9921\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.1547 - acc: 0.9392 - val_loss: 0.0229 - val_acc: 0.9934\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 33s 552us/step - loss: 0.1513 - acc: 0.9408 - val_loss: 0.0242 - val_acc: 0.9928\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 33s 544us/step - loss: 0.1518 - acc: 0.9402 - val_loss: 0.0312 - val_acc: 0.9907\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 32s 533us/step - loss: 0.1511 - acc: 0.9416 - val_loss: 0.0256 - val_acc: 0.9922\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 33s 548us/step - loss: 0.1516 - acc: 0.9402 - val_loss: 0.0252 - val_acc: 0.9919\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 32s 533us/step - loss: 0.1482 - acc: 0.9416 - val_loss: 0.0271 - val_acc: 0.9912\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 33s 552us/step - loss: 0.1461 - acc: 0.9421 - val_loss: 0.0280 - val_acc: 0.9914\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 33s 555us/step - loss: 0.1511 - acc: 0.9405 - val_loss: 0.0288 - val_acc: 0.9911\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.1478 - acc: 0.9429 - val_loss: 0.0236 - val_acc: 0.9931\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 33s 543us/step - loss: 0.1449 - acc: 0.9430 - val_loss: 0.0228 - val_acc: 0.9937\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 32s 533us/step - loss: 0.1487 - acc: 0.9417 - val_loss: 0.0223 - val_acc: 0.9932\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 33s 550us/step - loss: 0.1442 - acc: 0.9415 - val_loss: 0.0239 - val_acc: 0.9932\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 33s 542us/step - loss: 0.1433 - acc: 0.9430 - val_loss: 0.0246 - val_acc: 0.9928\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 33s 544us/step - loss: 0.1422 - acc: 0.9429 - val_loss: 0.0221 - val_acc: 0.9931\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 32s 537us/step - loss: 0.1406 - acc: 0.9433 - val_loss: 0.0238 - val_acc: 0.9925\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.1455 - acc: 0.9414 - val_loss: 0.0221 - val_acc: 0.9931\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 33s 549us/step - loss: 0.1441 - acc: 0.9408 - val_loss: 0.0245 - val_acc: 0.9932\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 33s 552us/step - loss: 0.1419 - acc: 0.9431 - val_loss: 0.0247 - val_acc: 0.9927\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 33s 546us/step - loss: 0.1402 - acc: 0.9441 - val_loss: 0.0248 - val_acc: 0.9922\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.1438 - acc: 0.9413 - val_loss: 0.0255 - val_acc: 0.9931\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.1408 - acc: 0.9424 - val_loss: 0.0234 - val_acc: 0.9927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe14d0c2550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-0IK5TFcBqL",
        "colab_type": "text"
      },
      "source": [
        "### what's the effect of adding Drop out after every BN for 80 ep aka HomeStretch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DJeeOGDU806",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2932
        },
        "outputId": "39d40c0c-a0f2-4c0b-ac29-82bb0fde809a"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=80, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/80\n",
            "60000/60000 [==============================] - 38s 628us/step - loss: 0.5788 - acc: 0.8656 - val_loss: 0.1857 - val_acc: 0.9675\n",
            "Epoch 2/80\n",
            "60000/60000 [==============================] - 32s 540us/step - loss: 0.2151 - acc: 0.9491 - val_loss: 0.0770 - val_acc: 0.9835\n",
            "Epoch 3/80\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.1549 - acc: 0.9621 - val_loss: 0.0629 - val_acc: 0.9841\n",
            "Epoch 4/80\n",
            "60000/60000 [==============================] - 33s 549us/step - loss: 0.1248 - acc: 0.9679 - val_loss: 0.0401 - val_acc: 0.9898\n",
            "Epoch 5/80\n",
            "60000/60000 [==============================] - 32s 527us/step - loss: 0.1077 - acc: 0.9718 - val_loss: 0.0409 - val_acc: 0.9886\n",
            "Epoch 6/80\n",
            "60000/60000 [==============================] - 34s 571us/step - loss: 0.0929 - acc: 0.9751 - val_loss: 0.0354 - val_acc: 0.9899\n",
            "Epoch 7/80\n",
            "60000/60000 [==============================] - 32s 529us/step - loss: 0.0867 - acc: 0.9761 - val_loss: 0.0354 - val_acc: 0.9894\n",
            "Epoch 8/80\n",
            "60000/60000 [==============================] - 32s 525us/step - loss: 0.0808 - acc: 0.9775 - val_loss: 0.0313 - val_acc: 0.9915\n",
            "Epoch 9/80\n",
            "60000/60000 [==============================] - 33s 545us/step - loss: 0.0751 - acc: 0.9791 - val_loss: 0.0310 - val_acc: 0.9924\n",
            "Epoch 10/80\n",
            "60000/60000 [==============================] - 32s 536us/step - loss: 0.0675 - acc: 0.9813 - val_loss: 0.0396 - val_acc: 0.9890\n",
            "Epoch 11/80\n",
            "60000/60000 [==============================] - 32s 540us/step - loss: 0.0686 - acc: 0.9807 - val_loss: 0.0288 - val_acc: 0.9914\n",
            "Epoch 12/80\n",
            "60000/60000 [==============================] - 32s 527us/step - loss: 0.0661 - acc: 0.9810 - val_loss: 0.0371 - val_acc: 0.9884\n",
            "Epoch 13/80\n",
            "60000/60000 [==============================] - 31s 520us/step - loss: 0.0606 - acc: 0.9830 - val_loss: 0.0305 - val_acc: 0.9911\n",
            "Epoch 14/80\n",
            "60000/60000 [==============================] - 33s 543us/step - loss: 0.0606 - acc: 0.9828 - val_loss: 0.0267 - val_acc: 0.9922\n",
            "Epoch 15/80\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.0578 - acc: 0.9837 - val_loss: 0.0270 - val_acc: 0.9912\n",
            "Epoch 16/80\n",
            "60000/60000 [==============================] - 34s 565us/step - loss: 0.0574 - acc: 0.9833 - val_loss: 0.0233 - val_acc: 0.9931\n",
            "Epoch 17/80\n",
            "60000/60000 [==============================] - 32s 525us/step - loss: 0.0553 - acc: 0.9843 - val_loss: 0.0266 - val_acc: 0.9921\n",
            "Epoch 18/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0548 - acc: 0.9838 - val_loss: 0.0301 - val_acc: 0.9907\n",
            "Epoch 19/80\n",
            "60000/60000 [==============================] - 33s 550us/step - loss: 0.0515 - acc: 0.9851 - val_loss: 0.0239 - val_acc: 0.9932\n",
            "Epoch 20/80\n",
            "60000/60000 [==============================] - 31s 520us/step - loss: 0.0517 - acc: 0.9851 - val_loss: 0.0287 - val_acc: 0.9908\n",
            "Epoch 21/80\n",
            "60000/60000 [==============================] - 32s 533us/step - loss: 0.0518 - acc: 0.9847 - val_loss: 0.0311 - val_acc: 0.9904\n",
            "Epoch 22/80\n",
            "60000/60000 [==============================] - 32s 529us/step - loss: 0.0496 - acc: 0.9858 - val_loss: 0.0249 - val_acc: 0.9930\n",
            "Epoch 23/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0491 - acc: 0.9857 - val_loss: 0.0230 - val_acc: 0.9929\n",
            "Epoch 24/80\n",
            "60000/60000 [==============================] - 33s 543us/step - loss: 0.0482 - acc: 0.9861 - val_loss: 0.0265 - val_acc: 0.9916\n",
            "Epoch 25/80\n",
            "60000/60000 [==============================] - 33s 547us/step - loss: 0.0479 - acc: 0.9858 - val_loss: 0.0241 - val_acc: 0.9925\n",
            "Epoch 26/80\n",
            "60000/60000 [==============================] - 32s 532us/step - loss: 0.0477 - acc: 0.9859 - val_loss: 0.0215 - val_acc: 0.9930\n",
            "Epoch 27/80\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.0477 - acc: 0.9860 - val_loss: 0.0204 - val_acc: 0.9932\n",
            "Epoch 28/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0443 - acc: 0.9867 - val_loss: 0.0228 - val_acc: 0.9934\n",
            "Epoch 29/80\n",
            "60000/60000 [==============================] - 33s 555us/step - loss: 0.0442 - acc: 0.9875 - val_loss: 0.0224 - val_acc: 0.9940\n",
            "Epoch 30/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0440 - acc: 0.9867 - val_loss: 0.0206 - val_acc: 0.9933\n",
            "Epoch 31/80\n",
            "60000/60000 [==============================] - 32s 528us/step - loss: 0.0427 - acc: 0.9874 - val_loss: 0.0232 - val_acc: 0.9932\n",
            "Epoch 32/80\n",
            "60000/60000 [==============================] - 32s 534us/step - loss: 0.0434 - acc: 0.9866 - val_loss: 0.0218 - val_acc: 0.9933\n",
            "Epoch 33/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0429 - acc: 0.9874 - val_loss: 0.0222 - val_acc: 0.9935\n",
            "Epoch 34/80\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.0422 - acc: 0.9874 - val_loss: 0.0206 - val_acc: 0.9937\n",
            "Epoch 35/80\n",
            "60000/60000 [==============================] - 33s 548us/step - loss: 0.0421 - acc: 0.9876 - val_loss: 0.0253 - val_acc: 0.9916\n",
            "Epoch 36/80\n",
            "60000/60000 [==============================] - 31s 524us/step - loss: 0.0395 - acc: 0.9881 - val_loss: 0.0213 - val_acc: 0.9930\n",
            "Epoch 37/80\n",
            "60000/60000 [==============================] - 32s 536us/step - loss: 0.0424 - acc: 0.9881 - val_loss: 0.0229 - val_acc: 0.9927\n",
            "Epoch 38/80\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.0425 - acc: 0.9872 - val_loss: 0.0265 - val_acc: 0.9922\n",
            "Epoch 39/80\n",
            "60000/60000 [==============================] - 33s 543us/step - loss: 0.0413 - acc: 0.9876 - val_loss: 0.0208 - val_acc: 0.9931\n",
            "Epoch 40/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0405 - acc: 0.9876 - val_loss: 0.0243 - val_acc: 0.9922\n",
            "Epoch 41/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0399 - acc: 0.9874 - val_loss: 0.0245 - val_acc: 0.9917\n",
            "Epoch 42/80\n",
            "60000/60000 [==============================] - 32s 540us/step - loss: 0.0404 - acc: 0.9877 - val_loss: 0.0226 - val_acc: 0.9928\n",
            "Epoch 43/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0397 - acc: 0.9885 - val_loss: 0.0228 - val_acc: 0.9931\n",
            "Epoch 44/80\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.0407 - acc: 0.9876 - val_loss: 0.0218 - val_acc: 0.9931\n",
            "Epoch 45/80\n",
            "60000/60000 [==============================] - 33s 546us/step - loss: 0.0398 - acc: 0.9880 - val_loss: 0.0208 - val_acc: 0.9936\n",
            "Epoch 46/80\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0383 - acc: 0.9888 - val_loss: 0.0223 - val_acc: 0.9933\n",
            "Epoch 47/80\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.0365 - acc: 0.9894 - val_loss: 0.0240 - val_acc: 0.9923\n",
            "Epoch 48/80\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 0.0364 - acc: 0.9888 - val_loss: 0.0215 - val_acc: 0.9932\n",
            "Epoch 49/80\n",
            "60000/60000 [==============================] - 32s 540us/step - loss: 0.0370 - acc: 0.9885 - val_loss: 0.0228 - val_acc: 0.9922\n",
            "Epoch 50/80\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0375 - acc: 0.9887 - val_loss: 0.0245 - val_acc: 0.9929\n",
            "Epoch 51/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0375 - acc: 0.9888 - val_loss: 0.0208 - val_acc: 0.9932\n",
            "Epoch 52/80\n",
            "60000/60000 [==============================] - 33s 542us/step - loss: 0.0382 - acc: 0.9886 - val_loss: 0.0240 - val_acc: 0.9925\n",
            "Epoch 53/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0364 - acc: 0.9888 - val_loss: 0.0230 - val_acc: 0.9926\n",
            "Epoch 54/80\n",
            "60000/60000 [==============================] - 33s 552us/step - loss: 0.0379 - acc: 0.9886 - val_loss: 0.0202 - val_acc: 0.9932\n",
            "Epoch 55/80\n",
            "60000/60000 [==============================] - 32s 538us/step - loss: 0.0361 - acc: 0.9889 - val_loss: 0.0207 - val_acc: 0.9943\n",
            "Epoch 56/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0356 - acc: 0.9894 - val_loss: 0.0203 - val_acc: 0.9935\n",
            "Epoch 57/80\n",
            "60000/60000 [==============================] - 33s 554us/step - loss: 0.0362 - acc: 0.9893 - val_loss: 0.0236 - val_acc: 0.9928\n",
            "Epoch 58/80\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0356 - acc: 0.9895 - val_loss: 0.0232 - val_acc: 0.9925\n",
            "Epoch 59/80\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.0360 - acc: 0.9893 - val_loss: 0.0217 - val_acc: 0.9932\n",
            "Epoch 60/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0359 - acc: 0.9893 - val_loss: 0.0233 - val_acc: 0.9927\n",
            "Epoch 61/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0354 - acc: 0.9894 - val_loss: 0.0198 - val_acc: 0.9942\n",
            "Epoch 62/80\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.0368 - acc: 0.9884 - val_loss: 0.0205 - val_acc: 0.9939\n",
            "Epoch 63/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0356 - acc: 0.9891 - val_loss: 0.0209 - val_acc: 0.9934\n",
            "Epoch 64/80\n",
            "60000/60000 [==============================] - 33s 548us/step - loss: 0.0345 - acc: 0.9894 - val_loss: 0.0204 - val_acc: 0.9942\n",
            "Epoch 65/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0362 - acc: 0.9887 - val_loss: 0.0219 - val_acc: 0.9931\n",
            "Epoch 66/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0357 - acc: 0.9889 - val_loss: 0.0187 - val_acc: 0.9946\n",
            "Epoch 67/80\n",
            "60000/60000 [==============================] - 33s 546us/step - loss: 0.0347 - acc: 0.9895 - val_loss: 0.0220 - val_acc: 0.9939\n",
            "Epoch 68/80\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0346 - acc: 0.9893 - val_loss: 0.0210 - val_acc: 0.9931\n",
            "Epoch 69/80\n",
            "60000/60000 [==============================] - 32s 537us/step - loss: 0.0352 - acc: 0.9895 - val_loss: 0.0223 - val_acc: 0.9932\n",
            "Epoch 70/80\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.0352 - acc: 0.9890 - val_loss: 0.0246 - val_acc: 0.9926\n",
            "Epoch 71/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0350 - acc: 0.9892 - val_loss: 0.0197 - val_acc: 0.9943\n",
            "Epoch 72/80\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.0333 - acc: 0.9899 - val_loss: 0.0202 - val_acc: 0.9940\n",
            "Epoch 73/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0331 - acc: 0.9896 - val_loss: 0.0191 - val_acc: 0.9942\n",
            "Epoch 74/80\n",
            "60000/60000 [==============================] - 34s 564us/step - loss: 0.0345 - acc: 0.9898 - val_loss: 0.0211 - val_acc: 0.9933\n",
            "Epoch 75/80\n",
            "60000/60000 [==============================] - 32s 525us/step - loss: 0.0341 - acc: 0.9898 - val_loss: 0.0192 - val_acc: 0.9941\n",
            "Epoch 76/80\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0342 - acc: 0.9892 - val_loss: 0.0187 - val_acc: 0.9942\n",
            "Epoch 77/80\n",
            "60000/60000 [==============================] - 33s 556us/step - loss: 0.0340 - acc: 0.9896 - val_loss: 0.0202 - val_acc: 0.9937\n",
            "Epoch 78/80\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0330 - acc: 0.9898 - val_loss: 0.0197 - val_acc: 0.9937\n",
            "Epoch 79/80\n",
            "60000/60000 [==============================] - 32s 532us/step - loss: 0.0347 - acc: 0.9893 - val_loss: 0.0188 - val_acc: 0.9943\n",
            "Epoch 80/80\n",
            "60000/60000 [==============================] - 32s 530us/step - loss: 0.0336 - acc: 0.9893 - val_loss: 0.0185 - val_acc: 0.9937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe14364e668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow1MVllFcjdW",
        "colab_type": "text"
      },
      "source": [
        "### ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "e2951b7c-16a3-40e6-963f-24150f8cbd38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.04965896717322466, 0.9861]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "63105130-a938-481a-982d-795f6a299ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.0593689e-17 6.3046323e-17 8.9005447e-13 1.8406407e-10 8.0754371e-19\n",
            "  8.0671096e-15 1.1510667e-24 1.0000000e+00 2.1262439e-10 2.7205176e-11]\n",
            " [2.5421015e-15 5.1460576e-12 1.0000000e+00 1.7606363e-12 6.3726551e-23\n",
            "  3.3248325e-16 4.4326671e-09 1.8222993e-27 1.8873889e-12 3.6800765e-21]\n",
            " [3.7568029e-08 9.9987435e-01 6.9531761e-06 6.3134886e-10 9.9738187e-05\n",
            "  5.2485571e-08 1.0483473e-07 1.0138228e-06 1.7757657e-05 1.9014761e-10]\n",
            " [1.0000000e+00 2.2739057e-15 6.6718070e-10 5.6225624e-15 4.5025926e-16\n",
            "  2.7407175e-13 1.3530634e-09 3.9892802e-13 5.8574483e-14 4.0855683e-12]\n",
            " [1.0115652e-12 6.9521903e-14 1.3467061e-13 1.9833676e-13 9.9999976e-01\n",
            "  4.2785103e-16 1.6022580e-12 5.5436229e-11 6.7551126e-10 2.2112354e-07]\n",
            " [2.3979660e-10 9.9982810e-01 3.5553690e-08 3.5910691e-11 1.0332796e-05\n",
            "  1.4169725e-10 2.0029900e-11 1.5314015e-04 8.3647601e-06 3.8377021e-10]\n",
            " [1.4182455e-19 9.8074493e-11 5.7774190e-09 2.3251079e-13 9.9966061e-01\n",
            "  6.0978769e-09 2.3871165e-16 2.2344653e-09 3.3876873e-04 5.9758122e-07]\n",
            " [7.2593749e-24 1.0864400e-09 7.5089712e-11 6.4676060e-12 5.3071453e-06\n",
            "  3.0447264e-08 5.8216405e-18 1.4018439e-14 9.3028554e-09 9.9999464e-01]\n",
            " [5.8572938e-11 5.5615773e-23 4.0553398e-19 1.0435163e-14 7.1205892e-15\n",
            "  3.1688964e-01 6.8311024e-01 4.5070697e-21 8.7986400e-08 1.6301742e-10]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7zrdWP8_E75",
        "colab_type": "code",
        "outputId": "f276f645-2371-4c57-d59c-cedee5cdc752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_74'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwoAAALUCAYAAACre8XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H/o0EvN9i749AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x864 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvo3WWsmHZNQ",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "---\n",
        "\n",
        "Model -   2 times 16, 3x3 -> BN -> 8, 1x1 -> BN (+) | MP | 2 times 16, 3x3 -> BN -> 8, 1x1 -> BN (+) | MP | 20, 3x3 -> BN -> 10, 1x1 -> 10, 2x2 {added BN after every 1x1}  </br>\n",
        "\n",
        "BS - 32 </br> \n",
        "Epoch - 30 </br>\n",
        "Parameters - 6624 </br>\n",
        "\n",
        "Train Acc -  0.9955 </br>\n",
        "Val Acc - 0.9909 </br>\n",
        "HIghest Val Acc - 0.9913 in 26th epoc</br>\n",
        "\n",
        "#### 2nd Run \n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 6624 </br>\n",
        "\n",
        "Train Acc -  0.9968 </br>\n",
        "Val Acc - 0.9898 </br>\n",
        "HIghest Val Acc - 0.9908 in 49th epoc</br>\n",
        "\n",
        "---\n",
        "\n",
        "Model 2 -   2 times 16, 3x3 -> BN -> 8, 1x1 -> BN | MP | 2 times 16, 3x3 -> BN -> 8, 1x1 -> BN | MP | 20, 3x3 -> BN -> 10, 1x1 -> BN (+) -> 10, 6x6 {adding BN after last 1x1 conv}  </br>\n",
        "\n",
        "BS - 32 </br> \n",
        "Epoch - 30 </br>\n",
        "Parameters - 6624 </br>\n",
        "\n",
        "Train Acc -  0.9956 </br>\n",
        "Val Acc - 0.9895 </br>\n",
        "HIghest Val Acc - 0.9916 in 27th epoc</br>\n",
        "\n",
        "\n",
        "#### 2nd Run \n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 6624 </br>\n",
        "\n",
        "Train Acc -  0.9972 </br>\n",
        "Val Acc - 0.9894 </br>\n",
        "HIghest Val Acc - 0.9904 in 34th epoc</br>\n",
        "\n",
        "---\n",
        "\n",
        "Model 2 -   2 times 16, 3x3 -> BN -> 8, 1x1 -> BN | MP | 2 times 16, 3x3 -> BN -> 8, 1x1 -> BN | MP | 20, 3x3 -> BN -> 10, 1x1 -> BN -> 10, 6x6 -> BN (+) -> flatten() {adding BN before flatten}  </br>\n",
        "\n",
        "BS - 32 </br> \n",
        "Epoch - 30 </br>\n",
        "Parameters - 6624 </br>\n",
        "\n",
        "Train Acc -  0.9941 </br>\n",
        "Val Acc - 0.9914 </br>\n",
        "HIghest Val Acc - <b>0.9929</b> in 21st epoc</br>\n",
        "\n",
        "\n",
        "#### 2nd Run \n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 6624 </br>\n",
        "\n",
        "Train Acc -  0.9966 </br>\n",
        "Val Acc - 0.9922 </br>\n",
        "HIghest Val Acc - 0.9924 in 34th epoc</br>\n",
        "\n",
        "\n",
        "#### Obs. \n",
        "Adding BN before flatten() layer does help, Highest Vald. Acc - <b> 0.9929 </b> achieved.\n",
        "\n",
        "\n",
        "#### MP-RM1\n",
        "\n",
        "Model 2 -   2 times 16, 3x3 -> BN -> 8, 1x1 -> BN | MP (-) | 2 times 16, 3x3 -> BN -> 8, 1x1 -> BN | MP | 20, 3x3 -> BN -> 10, 1x1 -> BN -> 10, 6x6 -> BN -> flatten() </br>\n",
        "\n",
        "BS - 32 </br> \n",
        "Epoch - 30 </br>\n",
        "Parameters - 6624 </br>\n",
        "\n",
        "Train Acc -  0.9941 </br>\n",
        "Val Acc - 0.9914 </br>\n",
        "HIghest Val Acc - 0.9929 in 21st epoc</br>\n",
        "\n",
        "\n",
        "#### 2nd Run \n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 12832 </br>\n",
        "\n",
        "Train Acc -  0.9966 </br>\n",
        "Val Acc - 0.9922 </br>\n",
        "HIghest Val Acc - 0.9924 in 34th epoc</br>\n",
        "\n",
        "#### MP-RM2\n",
        "\n",
        "Model 2 -   2 times 16, 3x3 -> BN -> 8, 1x1 -> BN | MP | 2 times 16, 3x3 -> BN -> 8, 1x1 -> BN | MP (-) | 20, 3x3 -> BN -> 10, 1x1 -> BN -> 10, 6x6 -> BN -> flatten() </br>\n",
        "\n",
        "BS - 32 </br> \n",
        "Epoch - 30 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9956 </br>\n",
        "Val Acc - 0.9923 </br>\n",
        "HIghest Val Acc - <b>0.9938</b> in 14th epoc</br>\n",
        "\n",
        "\n",
        "#### 2nd Run \n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 12832 </br>\n",
        "\n",
        "Train Acc -  0.9966 </br>\n",
        "Val Acc - 0.9922 </br>\n",
        "HIghest Val Acc - 0.9924 in 34th epoc</br>\n",
        "\n",
        "\n",
        "#### Obs. \n",
        "Max Pool in the middle of the model helped & Highest Val Acc <b>0.9938</b>\n",
        "\n",
        "#### Drop out results\n",
        "\n",
        "Model  -   2 times 16, 3x3 -> BN -> DP (+) -> 8, 1x1 -> BN -> DP (+) | MP | 2 times 16, 3x3 -> BN -> DP (+) -> 8, 1x1 -> BN -> DP (+) | MP (-) | 20, 3x3 -> BN -> DP (+) -> 10, 1x1 -> BN -> DP (+) -> 10, 6x6 -> BN -> flatten() </br>\n",
        "\n",
        "BS - 32 </br> \n",
        "Epoch - 30 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9870 </br>\n",
        "Val Acc - 0.9932 </br>\n",
        "HIghest Val Acc - 0.9940 in 27th epoc</br>\n",
        "\n",
        "\n",
        "#### 2nd Run \n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9889 </br>\n",
        "Val Acc - 0.9935 </br>\n",
        "HIghest Val Acc - <b> 0.9946 </b> in 25th epoc</br>\n",
        "\n",
        "#### 3rd Run BS 2x \n",
        "BS - 64 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9907 </br>\n",
        "Val Acc - 0.9946 </br>\n",
        "\n",
        "#### 4th Run BS 2x \n",
        "BS - 128 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9910 </br>\n",
        "Val Acc - 0.9939 </br>\n",
        "HIghest Val Acc - 0.9940 in 32nd epoc</br>\n",
        "\n",
        "#### 5th Run \n",
        "BS - 32 </br> \n",
        "Epoch - 80 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9893 </br>\n",
        "Val Acc - 0.9937 </br>\n",
        "HIghest Val Acc - <b>0.9946</b> in 66th epoc</br>\n",
        "\n",
        "Model 2 -   2 times 16, 3x3 -> BN -> DP -> 8, 1x1 -> BN -> DP | MP | 2 times 16, 3x3 -> BN -> DP -> 8, 1x1 -> BN -> DP | MP (-) | 20, 3x3 -> BN -> DP -> 10, 1x1 -> BN -> DP  -> 10, 6x6 -> BN -> DP (+) -> flatten() </br>\n",
        "added dp before flatten.\n",
        "\n",
        "BS - 32 </br> \n",
        "Epoch - 30 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9410 </br>\n",
        "Val Acc - 0.9921 </br>\n",
        "HIghest Val Acc - 0.9928 in 17th epoc</br>\n",
        "\n",
        "\n",
        "#### 2nd Run \n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9424 </br>\n",
        "Val Acc - 0.9927 </br>\n",
        "HIghest Val Acc - 0.9937 in 39th epoc</br>\n",
        "\n",
        "#### Obs. \n",
        "\n",
        "Drop after just before flatten doesn't help.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23lqcrxMc_iv",
        "colab_type": "text"
      },
      "source": [
        "##  Finally after the various trials I conclude\n",
        "\n",
        "*   BN helps the model\n",
        "*   Drop Out before flatten doesn't help the model rather it underfit's.\n",
        "\n",
        "*   Even i ran for more no of epoc it doesn't help.\n",
        "*   Maxpool in b/w the layer helped but if i tried to use Maxpool at the end doesn't help.\n",
        "\n",
        "\n",
        "*   when i comment initial MP it adds up to 12832 and when the same does for later MP it adds up to 10032, interestingly with less parameters the model able to reach good accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOqhI7yoareC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}