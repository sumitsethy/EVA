{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Third Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "jhQeEbRQmIuL",
        "F00GelGNmgEB",
        "hF3HvWt1mwK8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "353cad67-2512-4d05-f5ba-ec31975c1a9d"
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "7f0ec9cb-b955-4e95-df59-73c866588b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "28d22215-1aeb-4e3f-dd7c-a3488a4e51a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[12])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f90a454c240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ERfiAowg8E9X+RAYL329OsCgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "a9f0a2f7-d6cd-4702-c85e-c6c69a614412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "79f44f8d-7cfe-403f-ca76-be8da1c4160a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf_phDW-6tU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "910486fe-d72c-4d98-d09f-fbafc2e2e509"
      },
      "source": [
        "from keras.layers import Activation, BatchNormalization, Dropout\n",
        "model = Sequential()\n",
        "\n",
        "# After 1x1 BN added + mp rm1 \n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(8, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(8, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # mp-rm1\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(8, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(8, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2))) # mp-rm2\n",
        "\n",
        "model.add(Convolution2D(20, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1))\n",
        "model.add(BatchNormalization()) # **\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 6))\n",
        "model.add(BatchNormalization()) # *1\n",
        "# model.add(Dropout(0.1)) # exp dropout\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "623443b3-99e1-43f2-d077-7cb765b09583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1431
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_34 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 26, 26, 8)         136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 24, 24, 8)         136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 24, 24, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 24, 24, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 10, 10, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 10, 10, 8)         136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 10, 10, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 10, 10, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 8, 8, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 8, 8, 8)           136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 8, 8, 8)           32        \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 6, 6, 20)          1460      \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 6, 6, 20)          80        \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 6, 6, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 6, 6, 10)          210       \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 6, 6, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 1, 1, 10)          3610      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 10,032\n",
            "Trainable params: 9,760\n",
            "Non-trainable params: 272\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFsFwEy96kQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "sgd = optimizers.SGD(lr=0.3)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhQeEbRQmIuL",
        "colab_type": "text"
      },
      "source": [
        "### lr 0.1 + momentum + nestrov True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmJL9prc7Jtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2015
        },
        "outputId": "9d42f0a6-da7c-4fa1-c95f-2fa7be06959f"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 29s 488us/step - loss: 0.2364 - acc: 0.9281 - val_loss: 0.0663 - val_acc: 0.9798\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.1126 - acc: 0.9659 - val_loss: 0.0473 - val_acc: 0.9840\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 26s 438us/step - loss: 0.0931 - acc: 0.9718 - val_loss: 0.0351 - val_acc: 0.9889\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 25s 419us/step - loss: 0.0816 - acc: 0.9756 - val_loss: 0.0414 - val_acc: 0.9876\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0759 - acc: 0.9773 - val_loss: 0.0301 - val_acc: 0.9907\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 26s 437us/step - loss: 0.0719 - acc: 0.9780 - val_loss: 0.0310 - val_acc: 0.9892\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0670 - acc: 0.9800 - val_loss: 0.0331 - val_acc: 0.9899\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0639 - acc: 0.9804 - val_loss: 0.0306 - val_acc: 0.9893\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 26s 432us/step - loss: 0.0626 - acc: 0.9814 - val_loss: 0.0284 - val_acc: 0.9907\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0609 - acc: 0.9816 - val_loss: 0.0257 - val_acc: 0.9917\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0603 - acc: 0.9820 - val_loss: 0.0298 - val_acc: 0.9907\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0584 - acc: 0.9822 - val_loss: 0.0239 - val_acc: 0.9929\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 25s 424us/step - loss: 0.0547 - acc: 0.9837 - val_loss: 0.0244 - val_acc: 0.9923\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 26s 425us/step - loss: 0.0550 - acc: 0.9831 - val_loss: 0.0261 - val_acc: 0.9927\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0533 - acc: 0.9836 - val_loss: 0.0289 - val_acc: 0.9912\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 26s 438us/step - loss: 0.0514 - acc: 0.9844 - val_loss: 0.0208 - val_acc: 0.9934\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0506 - acc: 0.9848 - val_loss: 0.0234 - val_acc: 0.9919\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0498 - acc: 0.9850 - val_loss: 0.0242 - val_acc: 0.9920\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 26s 435us/step - loss: 0.0511 - acc: 0.9846 - val_loss: 0.0187 - val_acc: 0.9942\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0498 - acc: 0.9850 - val_loss: 0.0255 - val_acc: 0.9922\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0483 - acc: 0.9854 - val_loss: 0.0241 - val_acc: 0.9922\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0482 - acc: 0.9856 - val_loss: 0.0258 - val_acc: 0.9924\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0470 - acc: 0.9858 - val_loss: 0.0216 - val_acc: 0.9924\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 26s 441us/step - loss: 0.0463 - acc: 0.9856 - val_loss: 0.0211 - val_acc: 0.9938\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0468 - acc: 0.9857 - val_loss: 0.0199 - val_acc: 0.9942\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 25s 425us/step - loss: 0.0471 - acc: 0.9854 - val_loss: 0.0223 - val_acc: 0.9930\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0452 - acc: 0.9861 - val_loss: 0.0242 - val_acc: 0.9921\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 26s 433us/step - loss: 0.0447 - acc: 0.9859 - val_loss: 0.0238 - val_acc: 0.9923\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0440 - acc: 0.9864 - val_loss: 0.0223 - val_acc: 0.9930\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0429 - acc: 0.9868 - val_loss: 0.0226 - val_acc: 0.9926\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 25s 424us/step - loss: 0.0443 - acc: 0.9863 - val_loss: 0.0187 - val_acc: 0.9938\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 25s 425us/step - loss: 0.0446 - acc: 0.9860 - val_loss: 0.0237 - val_acc: 0.9928\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0407 - acc: 0.9877 - val_loss: 0.0221 - val_acc: 0.9933\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0405 - acc: 0.9873 - val_loss: 0.0215 - val_acc: 0.9933\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0396 - acc: 0.9879 - val_loss: 0.0209 - val_acc: 0.9929\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 26s 440us/step - loss: 0.0415 - acc: 0.9876 - val_loss: 0.0207 - val_acc: 0.9930\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0400 - acc: 0.9878 - val_loss: 0.0221 - val_acc: 0.9937\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 26s 437us/step - loss: 0.0403 - acc: 0.9874 - val_loss: 0.0211 - val_acc: 0.9936\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0416 - acc: 0.9870 - val_loss: 0.0225 - val_acc: 0.9933\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0410 - acc: 0.9871 - val_loss: 0.0197 - val_acc: 0.9940\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0399 - acc: 0.9876 - val_loss: 0.0280 - val_acc: 0.9911\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0386 - acc: 0.9885 - val_loss: 0.0223 - val_acc: 0.9931\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0383 - acc: 0.9882 - val_loss: 0.0215 - val_acc: 0.9931\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 26s 435us/step - loss: 0.0391 - acc: 0.9877 - val_loss: 0.0207 - val_acc: 0.9943\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0394 - acc: 0.9876 - val_loss: 0.0193 - val_acc: 0.9946\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0397 - acc: 0.9877 - val_loss: 0.0230 - val_acc: 0.9923\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0412 - acc: 0.9875 - val_loss: 0.0207 - val_acc: 0.9938\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 26s 432us/step - loss: 0.0381 - acc: 0.9887 - val_loss: 0.0208 - val_acc: 0.9934\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 26s 425us/step - loss: 0.0388 - acc: 0.9878 - val_loss: 0.0203 - val_acc: 0.9937\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0378 - acc: 0.9885 - val_loss: 0.0208 - val_acc: 0.9937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad699034e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F00GelGNmgEB",
        "colab_type": "text"
      },
      "source": [
        "### lr 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXxfwfGdBaAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1939
        },
        "outputId": "4b6f89cd-87a3-435a-fbc7-d523b997ea01"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 27s 442us/step - loss: 0.2996 - acc: 0.9198 - val_loss: 0.0786 - val_acc: 0.9776\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.1380 - acc: 0.9614 - val_loss: 0.0486 - val_acc: 0.9857\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 26s 429us/step - loss: 0.1093 - acc: 0.9695 - val_loss: 0.0427 - val_acc: 0.9867\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0955 - acc: 0.9731 - val_loss: 0.0408 - val_acc: 0.9875\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 24s 405us/step - loss: 0.0863 - acc: 0.9755 - val_loss: 0.0348 - val_acc: 0.9904\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 25s 409us/step - loss: 0.0788 - acc: 0.9770 - val_loss: 0.0324 - val_acc: 0.9904\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0758 - acc: 0.9788 - val_loss: 0.0293 - val_acc: 0.9916\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 26s 433us/step - loss: 0.0714 - acc: 0.9794 - val_loss: 0.0306 - val_acc: 0.9903\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0667 - acc: 0.9812 - val_loss: 0.0322 - val_acc: 0.9902\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 25s 423us/step - loss: 0.0639 - acc: 0.9813 - val_loss: 0.0279 - val_acc: 0.9921\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0618 - acc: 0.9823 - val_loss: 0.0253 - val_acc: 0.9926\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0590 - acc: 0.9826 - val_loss: 0.0328 - val_acc: 0.9902\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 25s 423us/step - loss: 0.0561 - acc: 0.9838 - val_loss: 0.0258 - val_acc: 0.9923\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 24s 401us/step - loss: 0.0597 - acc: 0.9826 - val_loss: 0.0239 - val_acc: 0.9931\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0553 - acc: 0.9838 - val_loss: 0.0225 - val_acc: 0.9930\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0540 - acc: 0.9844 - val_loss: 0.0251 - val_acc: 0.9921\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0532 - acc: 0.9845 - val_loss: 0.0266 - val_acc: 0.9920\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 24s 398us/step - loss: 0.0513 - acc: 0.9844 - val_loss: 0.0237 - val_acc: 0.9928\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0513 - acc: 0.9850 - val_loss: 0.0228 - val_acc: 0.9929\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 27s 455us/step - loss: 0.0500 - acc: 0.9857 - val_loss: 0.0236 - val_acc: 0.9928\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 24s 405us/step - loss: 0.0505 - acc: 0.9849 - val_loss: 0.0229 - val_acc: 0.9933\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0475 - acc: 0.9862 - val_loss: 0.0232 - val_acc: 0.9931\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0497 - acc: 0.9854 - val_loss: 0.0261 - val_acc: 0.9928\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0485 - acc: 0.9852 - val_loss: 0.0241 - val_acc: 0.9928\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0492 - acc: 0.9856 - val_loss: 0.0220 - val_acc: 0.9937\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0456 - acc: 0.9866 - val_loss: 0.0227 - val_acc: 0.9927\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 24s 399us/step - loss: 0.0450 - acc: 0.9871 - val_loss: 0.0222 - val_acc: 0.9928\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0466 - acc: 0.9862 - val_loss: 0.0225 - val_acc: 0.9932\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0453 - acc: 0.9868 - val_loss: 0.0236 - val_acc: 0.9927\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0442 - acc: 0.9870 - val_loss: 0.0258 - val_acc: 0.9916\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0446 - acc: 0.9869 - val_loss: 0.0241 - val_acc: 0.9921\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0447 - acc: 0.9869 - val_loss: 0.0230 - val_acc: 0.9926\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0445 - acc: 0.9866 - val_loss: 0.0230 - val_acc: 0.9925\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 24s 401us/step - loss: 0.0415 - acc: 0.9879 - val_loss: 0.0224 - val_acc: 0.9922\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0402 - acc: 0.9881 - val_loss: 0.0233 - val_acc: 0.9930\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 25s 422us/step - loss: 0.0429 - acc: 0.9866 - val_loss: 0.0211 - val_acc: 0.9936\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 24s 399us/step - loss: 0.0408 - acc: 0.9878 - val_loss: 0.0202 - val_acc: 0.9938\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 24s 399us/step - loss: 0.0413 - acc: 0.9876 - val_loss: 0.0218 - val_acc: 0.9932\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0424 - acc: 0.9875 - val_loss: 0.0241 - val_acc: 0.9919\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 24s 399us/step - loss: 0.0428 - acc: 0.9875 - val_loss: 0.0226 - val_acc: 0.9925\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 24s 401us/step - loss: 0.0409 - acc: 0.9879 - val_loss: 0.0225 - val_acc: 0.9928\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0412 - acc: 0.9877 - val_loss: 0.0237 - val_acc: 0.9914\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0395 - acc: 0.9883 - val_loss: 0.0212 - val_acc: 0.9932\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 24s 398us/step - loss: 0.0399 - acc: 0.9878 - val_loss: 0.0197 - val_acc: 0.9944\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0398 - acc: 0.9883 - val_loss: 0.0190 - val_acc: 0.9937\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 26s 433us/step - loss: 0.0384 - acc: 0.9889 - val_loss: 0.0185 - val_acc: 0.9937\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 24s 399us/step - loss: 0.0391 - acc: 0.9882 - val_loss: 0.0193 - val_acc: 0.9937\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0382 - acc: 0.9889 - val_loss: 0.0197 - val_acc: 0.9939\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0378 - acc: 0.9889 - val_loss: 0.0181 - val_acc: 0.9936\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0389 - acc: 0.9886 - val_loss: 0.0206 - val_acc: 0.9932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad3b5254e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF3HvWt1mwK8",
        "colab_type": "text"
      },
      "source": [
        "### lr 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9b7nSEfQ5qr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1939
        },
        "outputId": "718bf195-c634-440b-ba2a-640b98a77841"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 27s 446us/step - loss: 0.2493 - acc: 0.9310 - val_loss: 0.0602 - val_acc: 0.9817\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 25s 420us/step - loss: 0.1258 - acc: 0.9636 - val_loss: 0.0498 - val_acc: 0.9853\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.1036 - acc: 0.9694 - val_loss: 0.0389 - val_acc: 0.9875\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0901 - acc: 0.9733 - val_loss: 0.0357 - val_acc: 0.9892\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 26s 431us/step - loss: 0.0828 - acc: 0.9757 - val_loss: 0.0315 - val_acc: 0.9892\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0754 - acc: 0.9779 - val_loss: 0.0380 - val_acc: 0.9870\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0720 - acc: 0.9786 - val_loss: 0.0289 - val_acc: 0.9905\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0689 - acc: 0.9799 - val_loss: 0.0297 - val_acc: 0.9904\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 26s 432us/step - loss: 0.0673 - acc: 0.9797 - val_loss: 0.0278 - val_acc: 0.9910\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 24s 405us/step - loss: 0.0626 - acc: 0.9812 - val_loss: 0.0249 - val_acc: 0.9912\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 26s 425us/step - loss: 0.0609 - acc: 0.9817 - val_loss: 0.0232 - val_acc: 0.9922\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 24s 403us/step - loss: 0.0585 - acc: 0.9825 - val_loss: 0.0257 - val_acc: 0.9914\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0578 - acc: 0.9824 - val_loss: 0.0256 - val_acc: 0.9915\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0568 - acc: 0.9829 - val_loss: 0.0230 - val_acc: 0.9926\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0545 - acc: 0.9835 - val_loss: 0.0212 - val_acc: 0.9929\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 24s 403us/step - loss: 0.0539 - acc: 0.9841 - val_loss: 0.0223 - val_acc: 0.9928\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0544 - acc: 0.9836 - val_loss: 0.0227 - val_acc: 0.9925\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0509 - acc: 0.9848 - val_loss: 0.0204 - val_acc: 0.9937\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0516 - acc: 0.9843 - val_loss: 0.0227 - val_acc: 0.9927\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0510 - acc: 0.9848 - val_loss: 0.0210 - val_acc: 0.9933\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 27s 453us/step - loss: 0.0484 - acc: 0.9855 - val_loss: 0.0242 - val_acc: 0.9928\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0487 - acc: 0.9855 - val_loss: 0.0225 - val_acc: 0.9931\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0479 - acc: 0.9856 - val_loss: 0.0207 - val_acc: 0.9930\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0497 - acc: 0.9848 - val_loss: 0.0201 - val_acc: 0.9935\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 24s 403us/step - loss: 0.0482 - acc: 0.9856 - val_loss: 0.0200 - val_acc: 0.9938\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0479 - acc: 0.9857 - val_loss: 0.0193 - val_acc: 0.9934\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 25s 418us/step - loss: 0.0460 - acc: 0.9862 - val_loss: 0.0196 - val_acc: 0.9935\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0444 - acc: 0.9868 - val_loss: 0.0194 - val_acc: 0.9940\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 24s 401us/step - loss: 0.0444 - acc: 0.9859 - val_loss: 0.0199 - val_acc: 0.9932\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0457 - acc: 0.9862 - val_loss: 0.0202 - val_acc: 0.9933\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0434 - acc: 0.9870 - val_loss: 0.0209 - val_acc: 0.9934\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0438 - acc: 0.9862 - val_loss: 0.0189 - val_acc: 0.9938\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 24s 403us/step - loss: 0.0434 - acc: 0.9871 - val_loss: 0.0185 - val_acc: 0.9935\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 26s 433us/step - loss: 0.0419 - acc: 0.9873 - val_loss: 0.0209 - val_acc: 0.9935\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 24s 401us/step - loss: 0.0415 - acc: 0.9875 - val_loss: 0.0196 - val_acc: 0.9939\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 24s 403us/step - loss: 0.0415 - acc: 0.9872 - val_loss: 0.0193 - val_acc: 0.9933\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0418 - acc: 0.9876 - val_loss: 0.0208 - val_acc: 0.9939\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0411 - acc: 0.9875 - val_loss: 0.0203 - val_acc: 0.9938\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0406 - acc: 0.9880 - val_loss: 0.0208 - val_acc: 0.9929\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0397 - acc: 0.9879 - val_loss: 0.0199 - val_acc: 0.9943\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0397 - acc: 0.9880 - val_loss: 0.0190 - val_acc: 0.9940\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 24s 401us/step - loss: 0.0417 - acc: 0.9874 - val_loss: 0.0196 - val_acc: 0.9940\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0403 - acc: 0.9878 - val_loss: 0.0187 - val_acc: 0.9947\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 26s 438us/step - loss: 0.0409 - acc: 0.9879 - val_loss: 0.0185 - val_acc: 0.9945\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0411 - acc: 0.9873 - val_loss: 0.0190 - val_acc: 0.9939\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 25s 418us/step - loss: 0.0398 - acc: 0.9884 - val_loss: 0.0183 - val_acc: 0.9943\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 26s 438us/step - loss: 0.0395 - acc: 0.9877 - val_loss: 0.0201 - val_acc: 0.9934\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 24s 403us/step - loss: 0.0393 - acc: 0.9884 - val_loss: 0.0202 - val_acc: 0.9945\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0384 - acc: 0.9882 - val_loss: 0.0188 - val_acc: 0.9942\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 25s 424us/step - loss: 0.0392 - acc: 0.9880 - val_loss: 0.0198 - val_acc: 0.9937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad23a9e400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOndO_DvnJrI",
        "colab_type": "text"
      },
      "source": [
        "### lr 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl78Y-hYYMRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1927
        },
        "outputId": "5098b8df-5ec5-4a52-8330-a9ff9c508f23"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.2306 - acc: 0.9330 - val_loss: 0.0581 - val_acc: 0.9829\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.1149 - acc: 0.9657 - val_loss: 0.0517 - val_acc: 0.9837\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 26s 428us/step - loss: 0.0926 - acc: 0.9725 - val_loss: 0.0481 - val_acc: 0.9862\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 25s 419us/step - loss: 0.0823 - acc: 0.9752 - val_loss: 0.0317 - val_acc: 0.9902\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0745 - acc: 0.9777 - val_loss: 0.0367 - val_acc: 0.9884\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 25s 424us/step - loss: 0.0712 - acc: 0.9787 - val_loss: 0.0275 - val_acc: 0.9906\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 26s 436us/step - loss: 0.0663 - acc: 0.9801 - val_loss: 0.0240 - val_acc: 0.9923\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 24s 405us/step - loss: 0.0620 - acc: 0.9816 - val_loss: 0.0252 - val_acc: 0.9926\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0632 - acc: 0.9806 - val_loss: 0.0323 - val_acc: 0.9896\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0597 - acc: 0.9821 - val_loss: 0.0252 - val_acc: 0.9916\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0593 - acc: 0.9829 - val_loss: 0.0262 - val_acc: 0.9913\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0561 - acc: 0.9833 - val_loss: 0.0237 - val_acc: 0.9923\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 26s 428us/step - loss: 0.0563 - acc: 0.9827 - val_loss: 0.0244 - val_acc: 0.9924\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0536 - acc: 0.9837 - val_loss: 0.0286 - val_acc: 0.9905\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0523 - acc: 0.9842 - val_loss: 0.0212 - val_acc: 0.9934\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 26s 440us/step - loss: 0.0518 - acc: 0.9843 - val_loss: 0.0288 - val_acc: 0.9909\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0480 - acc: 0.9854 - val_loss: 0.0232 - val_acc: 0.9934\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0508 - acc: 0.9854 - val_loss: 0.0240 - val_acc: 0.9909\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 26s 428us/step - loss: 0.0479 - acc: 0.9857 - val_loss: 0.0236 - val_acc: 0.9923\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 26s 435us/step - loss: 0.0478 - acc: 0.9857 - val_loss: 0.0220 - val_acc: 0.9931\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 25s 409us/step - loss: 0.0480 - acc: 0.9857 - val_loss: 0.0212 - val_acc: 0.9933\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 25s 423us/step - loss: 0.0462 - acc: 0.9858 - val_loss: 0.0213 - val_acc: 0.9934\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0478 - acc: 0.9850 - val_loss: 0.0227 - val_acc: 0.9927\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0442 - acc: 0.9861 - val_loss: 0.0221 - val_acc: 0.9934\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 24s 408us/step - loss: 0.0461 - acc: 0.9857 - val_loss: 0.0212 - val_acc: 0.9934\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 26s 427us/step - loss: 0.0450 - acc: 0.9860 - val_loss: 0.0196 - val_acc: 0.9937\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0452 - acc: 0.9864 - val_loss: 0.0213 - val_acc: 0.9932\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 25s 411us/step - loss: 0.0449 - acc: 0.9865 - val_loss: 0.0208 - val_acc: 0.9931\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 26s 437us/step - loss: 0.0441 - acc: 0.9869 - val_loss: 0.0214 - val_acc: 0.9929\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 24s 408us/step - loss: 0.0431 - acc: 0.9866 - val_loss: 0.0181 - val_acc: 0.9942\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 25s 409us/step - loss: 0.0435 - acc: 0.9866 - val_loss: 0.0214 - val_acc: 0.9936\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 26s 439us/step - loss: 0.0430 - acc: 0.9868 - val_loss: 0.0222 - val_acc: 0.9934\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 24s 408us/step - loss: 0.0444 - acc: 0.9864 - val_loss: 0.0205 - val_acc: 0.9929\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0409 - acc: 0.9874 - val_loss: 0.0184 - val_acc: 0.9940\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 26s 429us/step - loss: 0.0419 - acc: 0.9872 - val_loss: 0.0191 - val_acc: 0.9941\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 25s 409us/step - loss: 0.0408 - acc: 0.9874 - val_loss: 0.0187 - val_acc: 0.9940\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0420 - acc: 0.9876 - val_loss: 0.0204 - val_acc: 0.9936\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0404 - acc: 0.9877 - val_loss: 0.0234 - val_acc: 0.9927\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 25s 423us/step - loss: 0.0379 - acc: 0.9881 - val_loss: 0.0211 - val_acc: 0.9934\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0389 - acc: 0.9883 - val_loss: 0.0223 - val_acc: 0.9932\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 25s 420us/step - loss: 0.0382 - acc: 0.9881 - val_loss: 0.0216 - val_acc: 0.9932\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 26s 429us/step - loss: 0.0382 - acc: 0.9879 - val_loss: 0.0191 - val_acc: 0.9943\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0385 - acc: 0.9886 - val_loss: 0.0214 - val_acc: 0.9932\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0380 - acc: 0.9881 - val_loss: 0.0199 - val_acc: 0.9938\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 27s 451us/step - loss: 0.0392 - acc: 0.9877 - val_loss: 0.0196 - val_acc: 0.9937\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 24s 408us/step - loss: 0.0375 - acc: 0.9888 - val_loss: 0.0215 - val_acc: 0.9930\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0391 - acc: 0.9884 - val_loss: 0.0168 - val_acc: 0.9950\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 26s 429us/step - loss: 0.0364 - acc: 0.9889 - val_loss: 0.0215 - val_acc: 0.9932\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 25s 409us/step - loss: 0.0379 - acc: 0.9886 - val_loss: 0.0211 - val_acc: 0.9941\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0385 - acc: 0.9883 - val_loss: 0.0182 - val_acc: 0.9942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad220691d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "c485076d-0795-425c-c8ab-414d88068230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.018212530983402395, 0.9942]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "63105130-a938-481a-982d-795f6a299ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.0593689e-17 6.3046323e-17 8.9005447e-13 1.8406407e-10 8.0754371e-19\n",
            "  8.0671096e-15 1.1510667e-24 1.0000000e+00 2.1262439e-10 2.7205176e-11]\n",
            " [2.5421015e-15 5.1460576e-12 1.0000000e+00 1.7606363e-12 6.3726551e-23\n",
            "  3.3248325e-16 4.4326671e-09 1.8222993e-27 1.8873889e-12 3.6800765e-21]\n",
            " [3.7568029e-08 9.9987435e-01 6.9531761e-06 6.3134886e-10 9.9738187e-05\n",
            "  5.2485571e-08 1.0483473e-07 1.0138228e-06 1.7757657e-05 1.9014761e-10]\n",
            " [1.0000000e+00 2.2739057e-15 6.6718070e-10 5.6225624e-15 4.5025926e-16\n",
            "  2.7407175e-13 1.3530634e-09 3.9892802e-13 5.8574483e-14 4.0855683e-12]\n",
            " [1.0115652e-12 6.9521903e-14 1.3467061e-13 1.9833676e-13 9.9999976e-01\n",
            "  4.2785103e-16 1.6022580e-12 5.5436229e-11 6.7551126e-10 2.2112354e-07]\n",
            " [2.3979660e-10 9.9982810e-01 3.5553690e-08 3.5910691e-11 1.0332796e-05\n",
            "  1.4169725e-10 2.0029900e-11 1.5314015e-04 8.3647601e-06 3.8377021e-10]\n",
            " [1.4182455e-19 9.8074493e-11 5.7774190e-09 2.3251079e-13 9.9966061e-01\n",
            "  6.0978769e-09 2.3871165e-16 2.2344653e-09 3.3876873e-04 5.9758122e-07]\n",
            " [7.2593749e-24 1.0864400e-09 7.5089712e-11 6.4676060e-12 5.3071453e-06\n",
            "  3.0447264e-08 5.8216405e-18 1.4018439e-14 9.3028554e-09 9.9999464e-01]\n",
            " [5.8572938e-11 5.5615773e-23 4.0553398e-19 1.0435163e-14 7.1205892e-15\n",
            "  3.1688964e-01 6.8311024e-01 4.5070697e-21 8.7986400e-08 1.6301742e-10]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7zrdWP8_E75",
        "colab_type": "code",
        "outputId": "16bea5b8-a7b6-4b45-c692-393c16796a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_42'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwoAAALUCAYAAACre8XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhhjjDGmC78oGGOMMcYYY7r4/9fhc4Rp1wI8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x864 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvo3WWsmHZNQ",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "---\n",
        "\n",
        "#### 0.1 + momentum + nesterov + SGD\n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9885 </br>\n",
        "Val Acc - 0.9937 </br>\n",
        "HIghest Val Acc - 0.9946 in 45th epoc</br>\n",
        "\n",
        "#### 0.1 + SGD\n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9886 </br>\n",
        "Val Acc - 0.9932 </br>\n",
        "HIghest Val Acc - 0.9944 in 44th epoc</br>\n",
        "\n",
        "#### 0.2 + SGD\n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9880 </br>\n",
        "Val Acc - 0.9937 </br>\n",
        "HIghest Val Acc - 0.9947 in 43rd epoc</br>\n",
        "\n",
        "#### 0.3 + SGD\n",
        "BS - 32 </br> \n",
        "Epoch - 50 </br>\n",
        "Parameters - 10032 </br>\n",
        "\n",
        "Train Acc -  0.9883 </br>\n",
        "Val Acc - 0.9942 </br>\n",
        "HIghest Val Acc - 0.9950 in 45th epoc</br>\n",
        "\n",
        "\n",
        "\n",
        "#### Obs. \n",
        "lr 0.3 achieve 0.9950 Val Acc (this is the best my model achieve)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOqhI7yoareC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}